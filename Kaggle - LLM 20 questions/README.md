# Kaggle - LLM for 20 questions

[Kaggle에서 진행한 스무고개 컴피티션](https://www.kaggle.com/competitions/llm-20-questions)



##### [진행 설명]

- 에이전트는 랜덤하게 2 vs 2 팀으로 맺어져, 질문과 답을 통해 스무고개의 답을 추론해가는 과정을 진행한다.
- 모든 대답은 `Yes` or `No` 로만 대답 가능하며, 에피소드 진행에 따른 추론을 진행한다.
- 에이전트는 `질문자_questioner`, `답변자_answerer`, `추론자_guesser`로 나누어지며, answer or guess의 값에 답이 나오면 스무고개를 종료한다.



##### [진행순서]

1. Started Code 분석
2. Input File 분석 및 Game Scenario 분석
3. RAG & 프롬프트 엔지니어링 공부
4. Local Test - llama3-8b, Gemma2-9b
5. Fine Tuning
6. Tokenizer의 이해
7. Information Gain by Questions (적용해보기)

---

##### [사용한 모델]

1. **Gemma-7b-it-quant**

   - 양자화 모델로 경량화를 거쳤고, 캐글에서 라이선스 승인 후 사용하기 쉬운 모델이다.
   - 사람들이 가장 많이 참고하는 스타터 코드에 사용된 모델이며, 리더보드 평균(LB600) 정도의 점수를 거두었다.
   - 생각보다 추론 확률이 좋지 않았다. 10게임 중 정답을 맞춘 게임이 고작 1~2판 정도임에도 성적을 올릴 수 있었던 건, 상대팀의 agent들이 자멸했기 때문.

   

2. **llama3-8b-chat**

   - keywords.py 안의 키워드를 Information gain에 따라 분류해보기 위해 사용한 모델.
   - 리더보드 상위 10~20%정도의 점수를 거두었다. (LB750)
   - 프롬프트 엔지니어링을 적용하여 제출한 모델이지만 여전히 정답을 많이 맞추진 못하였다.
     - 여전히 10판 하면 1~2판을 이기는 수준이었다.
     - 다만, 한 번 오류에 빠졌을 때, 같은 대답을 계속하진 않는 모델로 조금은 나아진 성능을 보였다.
   - 질문 순서는 "is it place" -> "is it man-made?" 를 기본으로 하며, 이 질문을 기반으로 좁혀나갈 수 있도록 프롬프트를 설계하였다. (일종의 rule-base)

   

3. **Gemma2-9b-it**

   - 조금 더 좋은 성능의 LLM을 찾아보다 적용한 모델로, 처음엔 캐글에서 사용할 수 없어 적용하기에 꽤나 애를 먹은 모델 (현재로서는 라이센스 승인만 받으면 쉽게 사용할 수 있는 것 같다.)
   - 모델이 커서 agent 제출이 되진 않았다. 다만, 캐글에서 가상의 agent를 만들어 셀프 스무고개를 진행했을 때, 꽤나 예측이 잘 된 모델이었다.
   - 만약 양자화 모델을 사용하여 경량화 할 수 있었다면, 더 좋은 성적을 낼 수 있지 않았을까 하는 아쉬움이 남는다.



##### [프로젝트 성과]

- LB500점대로 마무리 되어, 좋은 성적은 거두지 못하였다.
- LLM에 대한 이해 및 RAG, Fine Tuning에 대한 공부 진행



##### [기간]

프로젝트 기간

- 2024.05.16 - 2024.08.30

실제 참여 기간: 

- 2024.06.30 - 2024.08.30



##### [사용 기술]

- Python, Pandas, transformers, gemma (`license 승인 후 사용` )

