## 3. RAG 와 프롬프트 엔지니어링

### **1. RAG란?**

#### (Retrieval-Augmented Generation) - 검색, 증강, 생성

간단히 말하면 GPT에 정보검색 기능을 추가하는 것.

> 여기서 정보검색 내부 문서가 될 수도 있고, 인터넷 검색이 될 수도 있음.



### **RAG 기술이 주목받고 있는 이유?**

### **ChatGPT가 가질 수 있는 문제점을 해결 할 수 있기 때문.**

1. **최신 정보**에 대하여 학습되어 있지 않다.
2. **내부데이터**에 대한 학습이 되어 있지 않다.
3. 특정 도메인(개인정보 혹은 재부 정보)에 대한 기대하는 답변을 얻을 수 없다.
4. 문서의 양이 많아질 수록 **할루시네이션(환각) 현상**이 심해진다.

적합한 RAG를 적용하게 되면?

1. **최신 정보**를 기반으로 답변할 수 있으며, 정보를 찾을 수 없는 경우 "검색" 기능을 활용할 수 있다.
2. 내부데이터를 참고하여 답변할 수 있다.
3. 내부 DB에 저장할 수 있고, DB에 내용을 축적해 나갈 수 있으며, 저장된 DB에서 원하는 정보를 검색하여 검색된 정보를 바탕으로 답변받을 수 있다.
4. 할루시네이션 현상을 줄일 수 있다.

> 궁극적으로 더 나은 답변 품질을 기대할 수 있으며, 방대한 지식 기반으로 답변하는 도메인 특화 챗봇을 생성 할 수 있다. (우리가 축적해 나가는 DB 기준으로 특화된 챗봇을 만들 수 있게 된다.)



### **ChatGPT 안에 내장된 RAG?**

GPT에 문서를 업로드 한 후 질문을 하게 되면, 문서를 기반으로 답변을 하게 됨.

OpenAI는 RAG의 전반적인 과정(내부 내용)은 블랙박스로 되어있기 때문에 우리가 자세히 알 수 없음.

하지만, 문서 내부 구체적인 질문에는 제대로된 답변을 하지 못하거나, 관련 정보를 찾지 못하기도 함.

> 할루시네이션이 많이 일어나는 것 역시 문제.



ChatGPT의 **RAG는 우리가 컨트롤할 수 없는 영역**이기 때문에,

**유일한 최선은 "문서"를 GPT가 가장 잘 검색할 수 있는 형태로 변경하는 것.**

하지만, 모든 문서의 형태를 모두 변경하는 것은 사실상 어려운 일이다.



### **2. RAG 프로세스의 이해**

RAG가 차지하는 부분이 생각보다 크고, 이 부분을 한땀한땀 컨트롤한다면 더 좋은 결과가 나올 수 있다.

![RAG](..\assets\RAG.png)

### **3. 좋은 답변을 얻기 위해선?**

1. RAG (Context optimization - What the model need to know)
2. Prompt engineering
3. Fine-tuning (LLM optimization - the domain model specialize)
   - Full Fine-tuning이란 것도 있지만, 개인이 하는 단계에서는 사실상 불가능하다.

> 구현 난이도 :  prompt engineering <  RAG < Fine-tuning (가장 어려움)
>
> 최신 정보 반영: Fine-tuning < prompt engineering < RAG
>
> 유도과정 해석 : Fine-tuning & prompt engineering < RAG (Response Tracing이 가능하다.)



---

## 프롬프트 엔지니어링의 이해



### 1. 프롬프트란?

프롬프트는 AI 모델의 응답을 생성하기 위한 입력값이다.

프롬프트는 다음과 같은 4가지 구성 요소들로 이루어진다.

| **프롬프트 구성 요소**             | **의미**                                                     |
| ---------------------------------- | ------------------------------------------------------------ |
| **지시 (Instruction)**             | 모델이 수행할 특정 작업 또는 지시 사항                       |
| **문맥 (Context)**                 | 더 나은 응답을 위해 모델이 조종할 수 있는 외부 정보나 추가 문맥 |
| **입력 데이터 (Input Data)**       | 응답받고자 하는 입력이나 질문                                |
| **출력 지시지 (Output Indicator)** | 출력의 유형이나 형식을 나타내는 요소                         |

- 출처 : https://www.promptingguide.ai/



### 2. AI 모델의 하이퍼파라미터

`temperature`: **답변의 창의성, 무작위성에 관여.**

> 사실을 기반으로 하는 질의응답과 같은 작업에는 낮은 값(ex. 0.01)을 사용하여 보다 사실적이고 보다 일관된 응답을 얻을 수 있고, 시를 생성하는 등 창의적인 작업의 경우에는 값을 높이는 것(ex. 1)이 좋다.

`top_p`: **temperature와 함께 반응의 변동성을 관리하는 데 사용.**

> temparature가 확률 분포를 조정한다면, **top_p는 그 확률분포 내에서 선택할 단어의 범위를 결정한다.**  top_p가 낮을수록 응답의 정확도는 높아지지만 응답의 다양성은 낮아집니다. top_p가 높을수록 다양한 출력을 유도한다.
>
> - 나의 이해 : temperature와 매우 비슷해 보이지만 다르다. 일정 threshold값 이상만 답변으로 잡는다는 거 아닐까 싶다.

`top_k` : 대답하는 답변을 몇 개 받을 것인지를 결정. 일반적으로 기본 값이 0으로 설정하는 것을 권장한다고 한다. 자세한 이유가 궁금하신 경우 [네이버 클로바 팀의 문서](https://guide.ncloud-docs.com/docs/clovastudio-info#TopK)를 참조하시면 좋을 것 같다.

(가이드에서는 둘 중 하나만 변경하는 것을 권장한다.)

`Max Length`: max length를 조정하여 **모델이 생성하는 토큰의 수를 관리.**

> 답변이 지나치게 길어지는 것을 방지함으로써, 비용을 관리할 수 있다.

`Stop Sequences`: **모델의 토큰 생성을 중지하는 문자열**.

> stop sequences를 지정하는 것은 모델의 응답 길이 및 구조를 제어하는데 도움이 될 수 있다.
>
> 예를 들어, stop sequence로 "11"을 추가하여 항목이 10개를 초과하지 않는 리스트를 생성하도록 모델에 지시할 수 있다.

`Frequency Penalty`: 이 설정은 **모델이 동일한 단어나 구를 반복하지 못하도록 한다.**

> frequency penalty가 높을수록 단어가 다시 등장할 가능성이 줄어다. 자주 등장하는 토큰에 대하여 더 많은 페널티를 부여하여 모델의 응답에서 단어의 반복을 방지한다.

`Presence Penalty`: presence penalty는 **반복되는 토큰에 패널티를 적용하지만, frequency penalty와 달리 모든 토큰에 동일한 페널티가 적용됩니다.**

> 즉, 토큰이 2회 등장하는 토큰과 10회 등장하는 토큰이 동일한 페널티를 받는다.
>
> 이를 통해 다양한 답변이 나오도록 유도할 수 있으며, 더 다양하고 창의적인 텍스트를 원하면 증가, 집중된 콘텐츠를 원하면 낮출 수 있다.
>
> (temperature 및 top_p와 유사하게, 가이드에서는 frequency penalty와 presence penalty 중 하나만 변경하거나  다 변경하지 않는 것을 추천하고 있습니다.)