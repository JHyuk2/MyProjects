# 6. Tokenizer 이해



## Information Gain을 얻기 위한 모델 실행

저번 계획에서 말했던 것처럼, information gain을 통해 그 질문이 좋은 질문인지, 좋지 않은 질문인지를 판별. 그를 통해 프롬프트를 수정하고 모델을 실행시키는 과정에서, 토크나이저 사용법을 제대로 몰라 정리하게 되었다.

keyword와 question이 들어왔을 때, yes or no로 답변하는 `answerer` 만 필요하다.



#### **Prompt ver. 1**

- 아무것도 모르고, 기존 코드를 수정하여 작성한 프롬프트 (실행시 아무것도 출력 안됨)

```python
def template(obs):
    sys_prompt = f"""you are a helpful AI assistant, and your are very smart in judging correctness.
    the role of you is answering to question. 
    you must say 'yes' or 'no', any other answer is invalid.

    to help you, here's an example of how it should work assuming that the keyword is "Morocco":
    examle:
    <keyword: Morocco
    question: is it a place?
    you: "yes" >"""

    chat_template = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{sys_prompt}<|eot_id|>"""
    chat_template += "<|start_header_id|>user<|end_header_id|>\\n\\n"
    chat_template += f"keyword: '{obs.keyword}'\\n"
    chat_template += f"question: '{obs.questions[0]}' <|eot_id|>"
    return chat_template
```

시스템 프롬프트를 간소화하고, 내용을 수정했다.



요약) 너는 yes or no를 판단하는 유능한 AI, 나에게 정답을 알려줘!

```python
# pprint로 찍어본 chat_template
# obs.keyword : 'power cable'
# obs.questions : ['is it a place?']

('<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n'
 '\\n'
 'you are a helpful AI assistant, and your are very smart in judging '
 'correctness.\\n'
 '    the role of you is answering to question. \\n'
 "    you must say 'yes' or 'no', any other answer is invalid.\\n"
 '\\n'
 "    to help you, here's an example of how it should work assuming that the "
 'keyword is "Morocco":\\n'
 '    examle:\\n'
 '    <keyword: Morocco\\n'
 '    question: is it a place?\\n'
 '    you: "yes" ><|eot_id|><|start_header_id|>user<|end_header_id|>\\n'
 '\\n'
 "keyword: 'power cable'\\n"                # 입력한 obs 정보들
 "question: 'is it a place?' <|eot_id|>")
```

결론부터 말하자면...

이대로 출력했을 때 아무것도 나오지 않았고, 토크나이저에 대한 이해의 필요성을 느꼈다.

---



### 1. tokenizer 이해하기

```python
# 이미 설정된 토크나이저를 사용
tokenizer = AutoTokenizer.from_pretrained(model_id)
```

`tokenizer`를 프린트해보면 아래와 같이 나온다.

```
PreTrainedTokenizerFast(
 name_or_path='/kaggle/input/llama-3/transformers/8b-chat-hf/1',
 vocab_size=128000, model_max_length=1000000000000000019884624838656,
 is_fast=True, 
 padding_side='right', 
 truncation_side='right', 
 special_tokens={'bos_token': '<|begin_of_text|>', 'eos_token': '<|end_of_text|>'},
 clean_up_tokenization_spaces=True),  

--------------------------토큰--------------------------
 added_tokens_decoder={
	128000: AddedToken("<|begin_of_text|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	128001: AddedToken("<|end_of_text|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	...
	128006: AddedToken("<|start_header_id|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	128007: AddedToken("<|end_header_id|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	128008: AddedToken("<|reserved_special_token_4|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	128009: AddedToken("<|eot_id|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
```

`special_token`들을 사용해서 텍스트를 처리해야 한다는 건 대충 알겠다.

그 중에서도 특히 `bos_token(start of sentence), eos_token(end of sentence)` 두 개는 문맥에서 필수 요소이다.

하지만, 토큰들을 어떻게 사용하고 템플릿에 어떻게 활용되는지는 다른 것을 봐야 알 수 있다.

```python
# 클래스 사용법을 모를 땐, 일단 help를 통해 독스를 보는 것이 좋다.
help(tokenizer)
```

![tokenizer](/../assets/tokenizer_help.png)

중간에 자세히 보면, `chat_template`이라고 나와있고, 독스에 친절하게 설명해놓았다.

> 사이트 - [tokenizer.chat_template - HuggingFace](https://huggingface.co/docs/transformers/chat_templating)



### 2. 사용법 예시

**1) model.generate() 에 input하기 위한 사전 준비 -** `zephyr-7b-beta`

이해하기 쉬운 예시가 잘 나와있다.

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

checkpoint = "HuggingFaceH4/zephyr-7b-beta"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForCausalLM.from_pretrained(checkpoint)  # You may want to use bfloat16 and/or move to GPU here

messages = [
    {"role": "system",
     "content": "You are a friendly chatbot who always responds in the style of a pirate",},
     
    {"role": "user", 
    "content": "How many helicopters can a human eat in one sitting?"},
 ]
tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt")

# 출력 결과
>>> print(tokenizer.decode(tokenized_chat[0]))
"""
		<|system|>
		You are a friendly chatbot who always responds in the style of a pirate</s> 
		<|user|>
		How many helicopters can a human eat in one sitting?</s> 
		<|assistant|>
		Matey, I'm afraid I must inform ye that humans cannot eat helicopters. Helicopters are not food, they are flying machines. Food is meant to be eaten, like a hearty plate o' grog, a savory bowl o' stew, or a delicious loaf o' bread. But helicopters, they be for transportin' and movin' around, not for eatin'. So, I'd say none, me hearties. None at all.
"""
```

- messages 라는 리스트를 구성하여 tokenizer.apply_chat_template에 argument로 넘겨준다.

- messages 안에는 `role(system, user, assistant)`,  `content(텍스트)` 로 나눠져서 들어간다.

  - `system` : 프롬프트 엔지니어링 (너의 역할은 xx, 지금 해야 하는 일은 ooo)
  - `user` : 사용자 질문
  - `assistant` : 챗봇이 우리에게 주는 대답

- 즉, `tokenizer.apply_chat_template(message)` 를 통해 출력 결과와 같은 **string(chat template)을 생성**한다.

- 캐글에서 쉽게 사용하는 예시 (`transformers.pipeline`)

  > pipeline 예시가 있었지만, 로컬 환경에선 돌아가지 않아서 추가만 해놓는 코드
  >
  > ```python
  > from transformers import pipeline
  > 
  > pipe = pipeline("text-generation", model_id)
  > messages = [
  >     {
  >         "role": "system",
  >         "content": "You are a friendly chatbot who always responds in the style of a pirate",
  >     },
  >     {"role": "user", "content": "How many helicopters can a human eat in one setting?"},
  > ]
  > 
  > # Print the assistant's response
  > print(pipe(messages, max_new_tokens=128)[0]['generated_text'][-1])
  > ```

**2) generation prompt**

`tokenizer.apply_chat_template()` 을 다시 보면, `add_generation_prompt=True` 라는 argument가 선언되어있는데,

- 봇 응답의 시작을 위한 토큰을 추가하는 것을 의미한다.

**ex) add_generation_prompt = False**

```python
# 메세지 텍스트
messages = [
    {"role": "user", "content": "Hi there!"},
    {"role": "assistant", "content": "Nice to meet you!"},
    {"role": "user", "content": "Can I ask a question?"}
]

# 출력
# add_generation_prompt=False
tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
"""<|im_start|>user 
Hi there!<|im_end|>
<|im_start|>assistant
Nice to meet you!<|im_end|>
<|im_start|>user
Can I ask a question?<|im_end|>       # 유저 마지막 텍스트
"""

# add_generation_prompt=True
tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
"""<|im_start|>user
Hi there!<|im_end|>
<|im_start|>assistant
Nice to meet you!<|im_end|>
<|im_start|>user
Can I ask a question?<|im_end|>      # 유저 마지막 텍스트
<|im_start|>assistant                # 봇 응답의 시작 토큰 생성
"""
```

결국 LLM도 자연어 처리 모델의 확장판일 뿐이고, 이를 잘 사용하기 위해선 `토큰을 통한 가이드` 가 필요하다는 뜻!

~ BlenderBot, llama에서는 add_generation_prompt라던가 스페셜토큰이 없어도 된다고 하지만, 일단 넘어가자.



### 3. Advanced : Extra inputs to chat templates

apply_chat_template 을 사용하기 위해서는 “messages” 라는 인자가 필요하다. 하지만, 굳이 message를 넘기지 않고도 이를 사용할 수 있는 방법이 있다.

그것은 바로 `Jinja template` 을 이용하는 것!

- [Jinja template docs](https://jinja.palletsprojects.com/en/3.1.x/templates/)  // 일단 넘어가서 아래에서 이어서 설명

  

(다시 우리 코드로 돌아와서)

#### 1) llama 모델에서의 tokenizer.chat_template

> 아래의 모양을 가진 것이 바로 `jinja template.`

템플릿은 크게 세 가지 룰이 있다고 한다.

1. For each message, if the message is a user message, add a blank space before it, otherwise print nothing.

   각 메세지에 대해, 메세지 role이 user인 경우, 공백을 한 칸 추가하고 그렇지 않으면 아무것도 출력하지 않는다.

2. Add the message content

   메세지를 추가한다.

3. If the message is not the last message, add two spaces after it. After the final message, print the EOS token.

   마지막 메세지가 아닌 경우, 공백 두 칸을 추가한다. 마지막 메세지일 경우, EOS token을 출력



#### 2) Kaggle에서 보는 tokenizer.chat_template

```python
# tokenizer.chat_template 출력
{% set loop_messages = messages %}
{% for message in loop_messages %}
	{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}
	{% if loop.index0 == 0 %}
		{% set content = bos_token + content %}
	{% endif %}
	{{ content }}
{% endfor %}
{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}"""
```

- Kaggle에서 출력한 llama tokenizer.chat_template

```python
# llama tokenizer.chat_template의 구조
{% set loop_messages = messages %}         
{% for message in loop_messages %}
  {% set content = '' + message['role'] + '\\n\\n'+ message['content'] | trim + '' %}
  {% if loop.index0 == 0 %}                
    {% set content = bos_token + content %}
  {% endif %}
  {{ content }}
{% endfor %}
{{ 'assistant\\n\\n' }}
```

- 토큰이 없는 default_chat_template

토큰때문에 복잡해보이지만, 사실 큰 틀 자체는 default_chat_template과 동일하게 유지되고 있었다.





### 4. Chat Template 사용법_최종

1. messages 파라미터를 loop_messages로 설정

2. 모든 컨텐트는 아래의 형식을 따른다

   `<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>`

   - `\\n\\n` 두 줄을 추가하는 것으로, role과 content를 구분하고 있는 것 같다.

3. loop.index = 0인 경우(대화가 시작하는 경우), 시작 스페셜 토큰을 추가

   - content = bos_token(`begin_of_sentence`) + content

4. 마지막 메세지인 경우, 어시스턴트를 위한 대답 토큰을 추가

   어시스턴트가 답한 경우 대화가 종료되기 때문에, 마지막 스페셜 토큰을 추가

   - answerer의 chat_template에 eos_token(`end_of_sentence`) 추가.
   - assistant를 사용해 [생성하기 시작한 인덱스~ eos토큰 전]까지 생성된 답변을 output으로 사용.

------

기존 코드(`Prompt Ver.1`)를 가져와서 다시 보자면,

내가 설정한 answer는 몇 가지가 누락되어 있었다.

또한, 나는 한 개의 질문 후 대화를 종료할 것이기 때문에, eos토큰을 통해 대화를 종료시켜야 되는 게 아닐까?

- 대회에서는 assistant의 답변 후에도 대화가 이어지기에, eos 토큰이 아닌, <|eot_id|> 를 대신 사용하는 건가?

```
**## message_0
# index == 0 // bos 추가 및 role = system으로 설정 // '\\n\\n'을 통한 content 구분**
('<|begin_of_text|><|start_header_id|>**system**<|end_header_id|>\\n\\n'    
	
**# message['content']**
 'you are a helpful AI assistant, and your are very smart in judging '
 'correctness.\\n'
 '    the role of you is answering to question. \\n'
 "    you must say 'yes' or 'no', any other answer is invalid.\\n"
 '\\n'
 "    to help you, here's an example of how it should work assuming that the "
 'keyword is "Morocco":\\n'
 '    examle:\\n'
 '    <keyword: Morocco\\n'
 '    question: is it a place?\\n'
 '    you: "yes" >'

**# trim 후 <|eot_id|> 추가, 메세지 종료**
 '<|eot_id|>' 
-------------------------------------------------------
**## message_1**
**# index == 1 // role = user**
 <|start_header_id|>user<|end_header_id|>\\n\\n'
 
**# message['content']** 
 "keyword: 'power cable'\\n"                # 입력한 obs 정보들
 "question: 'is it a place?' 
**# <|eot_id|>**
 <|eot_id|>")
```

- assistant에게 응답을 받기 위한 토큰을 제공해주지 않음.
- 답변을 받은 후 eos 토큰 추가에 대해 작성하지 않음.

이 두 가지에 대해 수정을 거쳐 다시 함수를 작성해보자



## Prompt ver. 2

```python
# 대화 종료조건, assistant 답변 생성시 끊는 구간
id_eot = tokenizer.convert_tokens_to_ids(["<|eot_id|>"])[0]

# 원본에서 가져온 generate_answer
def generate_answer(template):
    inp_ids = tokenizer(template, return_tensors="pt").to("cuda")
    out_ids = model.generate(**inp_ids,max_new_tokens=15).squeeze()
    start_gen = inp_ids.input_ids.shape[1]
    out_ids = out_ids[start_gen:]
    if id_eot in out_ids:
        stop = out_ids.tolist().index(id_eot)
        out = tokenizer.decode(out_ids[:stop])
    else:
        out = tokenizer.decode(out_ids)
    return out
    
# 수정된 answerer
def answerer(obs):
    sys_prompt = f"""you are a helpful AI assistant, and your are very smart in judging correctness.
    the role of you is answering to question. 
    you must say 'yes' or 'no', any other answer is invalid.
    to help you, here's an example of how it should work assuming that the 'obs.keyword' is "Morocco":
    examle: 
    <user: is it a place?
    you: "yes" >"""

		# bos와 함께 system prompt 전달.
    chat_template = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{sys_prompt}<|eot_id|>"""
    chat_template += "<|start_header_id|>user<|end_header_id|>\\n\\n"
    
    # user message
    chat_template += f"'keyword is'{obs.keyword}, {obs.questions[0]}'<|eot_id|>"
    
    # assistant token
    chat_template += "<|start_header_id|>assistant<|end_header_id|>\\n\\n"
    
    # 모델에서 실제로 돌아가는 부분.
    output = generate_answer(chat_template)
    return output
```

이제 obs 객체를 생성해서 obs.keyword, obs.question[0]을 넘겨주면 된다.

```python
Class Observation:
	def __init__(self):
		self.step = 0
		self.role = 'answerer'
		self.turnType = 'answer'
		self.keyword = 'power cable'
		self.category = 'things'
		self.questions = ['is it a place?']
		self.answers = []
		self.guesses = []
		
obs = Observation()
```



### 코드 수정 후에 제대로 답을 얻은 모습.

![Untitled](/../assets/information_gain.png)

다만 응답하는데 시간이 조금 걸리는 게 마음에 걸리지만, information gain을 구해보자.

이제 가진 모든 keyword에 적용해보기

